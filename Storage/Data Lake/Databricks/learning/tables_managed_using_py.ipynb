{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804b7932-f0c1-4702-a1f1-84f7b21d85e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "categories_schema = StructType ([\n",
    "  StructField(\"categoryID\", IntegerType(),True),\n",
    "  StructField(\"categoryName\", StringType(), True),\n",
    "  StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format('csv').option(\"header\",True).schema(categories_schema).load(\"/Volumes/learning/test_schema/managed_volume/categories\")\n",
    "# If you know the table does not already exist, you can call this instead:\n",
    "df.write.saveAsTable(\"learning.test_schema.categories_delta_py\");\n",
    "\n",
    "# Create the table if it does not exist. Otherwise, replace the existing table.\n",
    "#df.writeTo(\"learning.test_schema.categories_delta_py\").createOrReplace();\n",
    "\n",
    "##### read\n",
    "df = spark.read.table(\"learning.test_schema.categories_delta_py\")\n",
    "df.show();\n",
    "display(df);\n",
    "\n",
    "###### update\n",
    "from delta.tables import *\n",
    "categories_dlt = DeltaTable.forName(spark,\"learning.test_schema.categories_delta_py\")\n",
    "categories_dlt.update(\n",
    "    condition = \"categoryID = 2\",\n",
    "    set = { \"categoryName\": \"'Updated'\" }\n",
    ")\n",
    "##### delete\n",
    "from delta.tables import *\n",
    "categories_dlt = DeltaTable.forName(spark, \"learning.test_schema.categories_delta_py\")\n",
    "categories_dlt.delete(\"categoryID = 2\")\n",
    "\n",
    "### history\n",
    "categories_dlt = DeltaTable.forName(spark, \"learning.test_schema.categories_delta_py\")\n",
    "display(categories_dlt.history())\n",
    "\n",
    "## time travel\n",
    "categories_dlt = DeltaTable.forName(spark, \"learning.test_schema.categories_delta_py\")\n",
    "deltaHistory = categories_dlt.history()\n",
    "display(deltaHistory.where(\"version == 1\"))\n",
    "\n",
    "## time travel read\n",
    "table_name = \"learning.test_schema.categories_delta_py\"\n",
    "df = spark.read.option('versionAsOf', 0).table(table_name)\n",
    "display(df)\n",
    "# optimize\n",
    "categories_dlt = DeltaTable.forName(spark, \"learning.test_schema.categories_delta_py\")\n",
    "categories_dlt.optimize().executeCompaction()\n",
    "categories_dlt.vacuum()\n",
    "\n",
    "\n",
    "\n",
    "categories_dlt.optimize().executeZOrderBy(\"categoryName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cc5338a-ea9e-4f5c-acd7-b7bb5a56d4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tables_managed_using_py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
